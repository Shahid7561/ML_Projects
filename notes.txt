Machine Learning:

What is model?
-> A trained algorithm which is:
    Used to identify patterns in your data,and 
    does not require explicit,manually set rules.

Three categories of ML:
1.Supervised Learning : Learn by identify the pattern in data that's already been labeled.
Types of Supervised Learning: 
    1.Classification: Predict the label for one of the predetermined categories. 
        Popular Classification Algorithms:
            1.Logistic Regression:
                Logistic regression models the probability of an instance belonging to a particular class. It's often used for binary classification.

            2.Naive Bayes:
                Naive Bayes uses probability and statistics to predict the probability of an input belonging to a certain class based on the presence of specific features.
                In Naive Bayes we assume that all the features are independent.
                Naive: "naive" means simple or basic.
                Bayes: Named after the mathematician Thomas Bayes, this algorithm uses Baye's theorem,it calculates the probability of a data point belonging to a certain class given its features.
                P(A/B) = P(B/A) * P(A)/P(B)
                P Stands for probability,A and B are two different class or features.

                Types of naive bayes classifiers:
                    1.Gaussian Naive Bayes:
                    Assumption: It assumes that the features follow a Gaussian (normal) distribution.
                    Use Cases: Typically used when dealing with continuous data, such as sensor readings or measurements.

                    2.Multinomial Naive Bayes:
                    Assumption: It assumes that the features follow a multinomial distribution, which is often the case when dealing with discrete data, like text data where you're counting word occurrences.
                    Use Cases: Widely used in text classification tasks, such as spam detection and document categorization.

                    3.Bernoulli Naive Bayes:
                    Assumption: Assumes that the features are binary, i.e., they can take on only two values (0 or 1).
                    Use Cases: Commonly used in situations where you have binary data like presence or absence of certain features, such as sentiment analysis (positive/negative sentiment) or document classification (word presence/absence).

            3.K-Nearest Neighbors:
                KNN classifies data points based on the majority class of their nearby neighbors. The "k" refers to the number of neighbors considered.

            4.Decision Tree:
                Decision trees ask a series of questions to classify data. Each question narrows down the possibilities until the algorithm decides on a category.

            5.Support Vector Machines:
                SVM finds the best line (or surface in higher dimensions) to separate data into classes with the maximum distance between them.

            6.Random Forest:
                A random forest combines the decisions of multiple decision trees to improve accuracy and make more reliable classifications.

    2.Regression: Predict the continues value
        Popular Regression Algorithms:
            1.Linear Regression:
                Linear regression finds the best-fitting linear equation to predict a numeric outcome. It's like drawing a line that minimizes the distance between the points and the line itself.
                Y = mx + b is a formula to predict the value in single linear regression.
                Y = predicted value (dependent variable)
                m = coeficent
                x = feature (independent variable)
                b = interception

                Y = m1x1 * m2x2 * m3x3 ... mnxn + b is a formula to predict the value in linear regression with multiple variable.

            2.Decision Tree Regression:
                Decision tree regression uses a tree structure to predict numeric outcomes. It breaks down the data into smaller subsets by asking questions about different features.

            3.Random Forest Regression:
                Random forest regression combines the predictions of many decision trees to improve accuracy and account for different possibilities in the data.
                
            4.Support Vector Regression:
                SVR extends the idea of support vector machines to regression. It finds a line (or surface in higher dimensions) that best fits the data points while allowing a certain margin of error.

        
2.Unsupervised Learning: There is no labeled data and model learns the data than identify the pattern itself.
Types of Unsupervised Learning:
    1.Clustering: Identify the data and make groups of similar data and make clusters of groups.

        Types of Clustering:
        1.K Means Clustering:K Means clustering algorithm is unsupervised machine learning technique used to cluster data points. 
        
    2.Association:Identify the data and according to past data model suggest about new data.

3.Reinforcement: Learn through trial and error, The model recieve reward
if output is correct else model receive penatly.
if there are more reward and less penatly than you assume that your model
is good.

ML pipeline:
1.Identify Problem
2.Problem formulation
3.Daṭa collection and integration
4.Data preprocessing and visulization
5.Feature engineering
6.Model training and tuning
7.Model Evalution
8.Model Deployment  

Gradient Descent: Optimizing parameter adjustments to minimize errors.
The goal is to minimize a value called the cost function or loss function. This function quantifies how far off the model's predictions are from the actual targets (labels) in the training data.
The gradient tells us the direction and magnitude of the steepest increase in the cost function.


Cost Function (Loss Function):The cost function (or loss function) is a mathematical measure of how well a model's predictions match the actual targets in the training data. 
It quantifies the difference between predicted values and actual values. The lower the cost, the better the model's predictions align with the truth.

Hyper parameter tuning: The process of choosing the optimal parameter 

---------------------------------------------------------------------------

Deep Learning: Deep Learning is a subset of Machine Learning that involves neural networks with many interconnected layers.

Neurons: Basic processing units that mimic brain cells.
Layers: Neurons are organized into input, hidden, and output layers.
Activation: Neurons apply activation functions to inputs, determining their output.

    Mostly used activation functions:
    1.Sigmoid Activation Function: 
        Formula: σ(x) = 1 / (1 + e^(-x))
        Output Range: (0, 1)
        Characteristics:
        S-shaped curve
        Suitable for binary classification problems

    2.Hyperbolic Tangent (tanh) Activation Function:
        Formula: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
        Output Range: (-1, 1)
        Characteristics:
        Similar to the sigmoid but centered around zero

    3.Rectified Linear Unit (ReLU):
        Formula: ReLU(x) = max(0, x)
        Output Range: (0, ∞)
        Characteristics:
        Simple and computationally efficient
        
Dense: All Neurons are connected with all other Neurons in other layers

Deep Structure: Multiple hidden layers for complex pattern recognition.
Feature Learning: Automatically learning useful features from raw data.

Popular Neural Networks:
Convolutional Neural Networks (CNNs): Excellent for image recognition due to their ability to capture spatial hierarchies.

Recurrent Neural Networks (RNNs): Suited for sequences, like time series or language, due to their memory of previous inputs.

Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs): RNN variants for better handling long-range dependencies.

Backward error propagation: also known as backpropagation,that enables neural networks to learn from their mistakes and adjust their internal parameters (weights and biases) to minimize the error in their predictions. 

Overfitting: When a network learns noise in data instead of true patterns. Regularization techniques help prevent this.

Convolutional Neural Networks (CNNs):
1.Convolution is the heart of CNNs. It's a mathematical operation that involves sliding a small filter (also called a kernel) over an image and calculating dot products between the filter and the image portions it covers.
The purpose of convolution is to extract features from the image, like edges, corners, textures, and other patterns.

2.Activation functions, like ReLU (Rectified Linear Activation), introduce non-linearity to the network, allowing it to capture complex relationships in the data.

3.Pooling is a downsampling operation that reduces the spatial dimensions of the feature maps while retaining important information.
Common pooling methods include max pooling (selecting the maximum value in each pool) and average pooling (calculating the average value).
Pooling helps make the network more robust to variations in the input and reduces computational complexity.

4.After the convolutional and pooling layers, CNNs often have one or more fully connected layers (also known as dense layers).
These layers take the high-level features learned by the convolutional layers and make predictions based on them.


Recurrent Neural Networks (RNNs):They are particularly well-suited for tasks where the order and context of data elements matter, such as natural language processing, speech recognition, time series analysis, and more. RNNs have a unique architecture that allows them to maintain hidden states and capture information from previous time steps, making them powerful for modeling sequences.

1.Recurrent Connections:RNNs have recurrent connections within the network, which means they can take input not only from the current time step but also from previous time steps in the sequence.

2.Hidden State:RNNs maintain a hidden state vector that serves as memory to store information from previous time steps. This hidden state is updated at each time step based on the current input and the previous hidden state.

3.Input and Output Sequences:An RNN processes sequences of data. Each time step in the sequence corresponds to an input and output. For example, in natural language processing, each time step might represent a word, and the output could be the predicted next word.

4.Weight Sharing:In an RNN, the same set of weights and biases are shared across all time steps. This allows the model to learn and apply the same transformations to each element in the sequence, maintaining a form of parameter sharing.

5.Vanishing Gradient Problem:RNNs are susceptible to the vanishing gradient problem, which occurs when gradients become too small during backpropagation through time (BPTT). This problem can make it challenging for RNNs to capture long-range dependencies in data.

6.Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU):To address the vanishing gradient problem, more advanced RNN variants like LSTMs and GRUs were introduced. These architectures have gating mechanisms that control the flow of information in and out of the hidden state, allowing them to capture long-term dependencies more effectively.

Applications of RNN:
    1.Translation in other languages
    2.Named Entity Recognition
    3.Auto complete words
    4.Sentiment Analysis
    5.Generation of words